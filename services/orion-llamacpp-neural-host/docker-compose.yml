# services/orion-llamacpp-neural-host/docker-compose.yml

services:
  orion-llamacpp-neural-host:
    build:
      context: ../..
      dockerfile: services/orion-llamacpp-neural-host/Dockerfile
    env_file:
      - .env
    image: orion-llamacpp-neural-host:0.1.0
    container_name: ${PROJECT:-orion}-orion-llamacpp-neural-host
    restart: unless-stopped

    environment:
      # --- Identity & Bus ---
      - SERVICE_NAME=${SERVICE_NAME:-orion-llamacpp-neural-host}
      - SERVICE_VERSION=${SERVICE_VERSION:-0.1.0}
      - ORION_BUS_URL=${ORION_BUS_URL:-redis://redis:6379/0}
      - NODE_NAME=${NODE_NAME:-unknown}
      - INSTANCE_ID=${INSTANCE_ID:-default}

      # --- Profiles & Selection ---
      - LLM_PROFILE_NAME=${LLM_PROFILE_NAME}
      - LLM_PROFILES_CONFIG_PATH=${LLM_PROFILES_CONFIG_PATH:-/app/config/llm_profiles.yaml}
      - HF_TOKEN=${HF_TOKEN}

      # --- Overrides (Optional) ---
      - LLAMACPP_MODEL_PATH_OVERRIDE=${LLAMACPP_MODEL_PATH_OVERRIDE}
      - LLAMACPP_HOST_OVERRIDE=${LLAMACPP_HOST_OVERRIDE}
      - LLAMACPP_PORT_OVERRIDE=${LLAMACPP_PORT_OVERRIDE}
      - LLAMACPP_CTX_SIZE_OVERRIDE=${LLAMACPP_CTX_SIZE_OVERRIDE}
      - LLAMACPP_N_GPU_LAYERS_OVERRIDE=${LLAMACPP_N_GPU_LAYERS_OVERRIDE}
      - LLAMACPP_THREADS_OVERRIDE=${LLAMACPP_THREADS_OVERRIDE}
      - LLAMACPP_N_PARALLEL_OVERRIDE=${LLAMACPP_N_PARALLEL_OVERRIDE}
      - LLAMACPP_BATCH_SIZE_OVERRIDE=${LLAMACPP_BATCH_SIZE_OVERRIDE}
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}

      # Deprecated but kept for compatibility during migration if needed
      - LLAMACPP_PORT=${LLAMACPP_PORT:-8005}

    volumes:
      # Mount the cache directory to /models
      - "${LLM_CACHE_DIR}:/models"

    ports:
      - "${LLAMACPP_PORT:-8005}:${LLAMACPP_PORT:-8005}"

    networks:
      - app-net

    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

networks:
  app-net:
    external: true
