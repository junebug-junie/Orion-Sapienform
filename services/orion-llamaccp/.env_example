 cat services/orion-llamaccp/.env
# services/orion-llamaccp/.env

SERVICE_NAME=orion-llamaccp
SERVICE_VERSION=0.1.0

# HTTP bind inside the container (llama-server defaults to 0.0.0.0:8080)
LLAMACPP_HOST=0.0.0.0
LLAMACPP_PORT=8080
LLAMACPP_HOST_PORT=7005

# Orion Bus (used by llm-gateway, not this engine directly)
ORION_BUS_URL=redis://orion-bus:6379/0
ORION_BUS_ENABLED=true

# Profiles config for Orion (used by llm-gateway & other services, not this engine)
LLM_PROFILES_CONFIG_PATH=/app/config/llm_profiles.yaml
LLM_PROFILE_NAME=deepseek-70b-gguf-atlas
LLM_MODEL_ID=

# Path to your GGUF model inside the container
LLAMACPP_MODEL=/models/gguf/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf

# llama.cpp runtime knobs (these become LLAMA_ARG_* for the server)
LLAMACPP_CTX_SIZE=8192
LLAMACPP_N_GPU_LAYERS=80
LLAMACPP_THREADS=16
LLAMACPP_N_PARALLEL=2
LLAMACPP_BATCH_SIZE=512

# Pin GPUs (matches your V100 layout on Atlas)
CUDA_VISIBLE_DEVICES=0,1,2,3
