# ───────────────────────────────────────────────────────────────
# 🧠 Orion Brain — Cognitive Core
# Manages inference, routing, and event streams across the Mesh
# ───────────────────────────────────────────────────────────────

x-env-context: &env
  env_file:
    - /mnt/services/Orion-Sapienform/.env   # mesh root (PROJECT, NET, etc.)
    - .env                                  # local overrides (brain-specific)

services:
  brain:
    <<: *env
    build:
      context: ../..
      dockerfile: services/orion-brain/Dockerfile
    image: ${PROJECT:-orion}-brain:latest
    container_name: ${PROJECT:-orion}-brain
    ports:
      - "${PORT:-8088}:8088"
    restart: unless-stopped
    environment:
      BACKENDS: ${BACKENDS:-http://llm-brain:11434}
      SELECTION_POLICY: ${SELECTION_POLICY:-least_conn}
      HEALTH_INTERVAL_SEC: ${HEALTH_INTERVAL_SEC:-5}
      CONNECT_TIMEOUT_SEC: ${CONNECT_TIMEOUT_SEC:-10}
      READ_TIMEOUT_SEC: ${READ_TIMEOUT_SEC:-600}

      PORT: ${PORT:-8088}
      SERVICE_NAME: ${SERVICE_NAME:-brain}
      ORION_BUS_ENABLED: ${ORION_BUS_ENABLED:-true}

      # 🔥 Correct dynamic link to bus-core
      REDIS_URL: ${REDIS_URL:-redis://${PROJECT}-bus-core:6379/0}

      EVENTS_ENABLE: ${EVENTS_ENABLE:-true}
      EVENTS_STREAM: ${EVENTS_STREAM:-orion:evt:gateway}
      BUS_OUT_ENABLE: ${BUS_OUT_ENABLE:-true}
      BUS_OUT_STREAM: ${BUS_OUT_STREAM:-orion:bus:out}
      NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-all}

    # 🕐 Add startup delay so bus DNS is ready
    command: ["/bin/sh", "-c", "sleep ${STARTUP_DELAY:-5} && python -m uvicorn app.main:app --host 0.0.0.0 --port ${PORT:-8088}"]

    networks:
      app-net:
        aliases:
          - brain
          - ${PROJECT:-orion}-brain
          - llm-brain

  brain-llm:
    <<: *env
    image: ollama/ollama:latest
    container_name: ${PROJECT:-orion}-brain-llm
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
    dns:
      - 1.1.1.1
      - 8.8.8.8
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    networks:
      app-net:
        aliases:
          - llm-brain

volumes:
  ollama_data:

networks:
  app-net:
    external: true
