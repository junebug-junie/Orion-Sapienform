# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ðŸ§  Orion LLM Gateway â€” bus-native model client
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SERVICE_NAME=llm-gateway
SERVICE_VERSION=0.1.0

# Bus (usually already defined at project root, but safe to override here)
ORION_BUS_ENABLED=true
ORION_BUS_URL=redis://100.92.216.81:6379/0

# LLM gateway bus channels
CHANNEL_LLM_INTAKE=orion-exec:request:LLMGatewayService
CHANNEL_LLM_REPLY_PREFIX=orion:llm:reply

# LLM defaults (legacy path when profiles are not used)
ORION_DEFAULT_LLM_MODEL=llama3.1:8b-instruct-q8_0
ORION_LLM_DEFAULT_BACKEND=ollama

# Backend endpoints
# Today: Ollama running as `brain-llm` on app-net
ORION_LLM_OLLAMA_URL=http://brain-llm:11434


# Future: vLLM (OpenAI-style) on your V100 carrier board
# Uncomment and set when you bring vLLM online
# ORION_LLM_VLLM_URL=http://orion-vllm:8000
ORION_LLM_VLLM_URL=

# Optional: call Brain's /chat (Spark, Ï†, collapse) as a backend
# ORION_LLM_BRAIN_URL=http://brain:8088
ORION_LLM_BRAIN_URL=

# Timeouts (seconds)
CONNECT_TIMEOUT_SEC=10
READ_TIMEOUT_SEC=60

# Service label in replies
ORION_LLM_SERVICE_NAME=LLMGatewayService

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ðŸ”§ LLM Profiles (vLLM / brain / ollama routing)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Path *inside* the container where the profiles YAML will be mounted
LLM_PROFILES_CONFIG_PATH=/app/config/llm_profiles.yaml

# Default profile name to use when no explicit profile_name is provided
# and we can't pick one via verb. Must match a key in config/llm_profiles.yaml.
LLM_DEFAULT_PROFILE_NAME=llama3-7b-v100-1gpu
