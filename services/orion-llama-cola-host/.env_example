SERVICE_NAME=llama-cola-host
SERVICE_VERSION=0.1.0
NODE_NAME=athena
INSTANCE_ID=llama-cola-1

ORION_BUS_URL=redis://orion-redis:6379/0
ORION_BUS_ENABLED=true
ORION_BUS_ENFORCE_CATALOG=true

# Which profile this service should boot
LLM_PROFILES_CONFIG_PATH=/app/config/llm_profiles.yaml
LLM_PROFILE_NAME=llama3-1-cola
#qwen2.5-32b-instruct-abliterated-gguf
#deepseek-70b-gguf-atlas

# Container mount root for models volume (compose mounts host cache -> /models)
MODELS_MOUNT_ROOT=/models
LLM_CACHE_DIR=/mnt/telemetry/llm-cache

# Optional: HuggingFace token (only needed for gated/private)
HF_TOKEN=
hf_token=

# Optional: runtime overrides (ONLY if you want to override the profile YAML)
# (These are your “safe boot” levers if the model crashes at 8192/parallel/etc.)
LLAMA_COLA_MODEL_PATH_OVERRIDE=
LLAMA_COLA_REVISION_OVERRIDE=

# Optional: override GPU pinning (otherwise uses profile.gpu.device_ids)
CUDA_VISIBLE_DEVICES_OVERRIDE=

# Behavior
ENSURE_MODEL_DOWNLOAD=true
WAIT_FOR_MODEL_SECONDS=0

# Host port mapping (used by compose for published port)
LLAMA_COLA_HOST_PORT=8005
