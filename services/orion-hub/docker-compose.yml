services:

  hub-app:
    build:
      context: ../..
      dockerfile: services/orion-hub/Dockerfile
    container_name: ${PROJECT}-hub

    # use when on atlas:
    network_mode: "host"

    # use when on athena:
    #ports:
    #  - "${HUB_PORT}:${HUB_PORT}" # if running brain and hub on single node

    restart: unless-stopped
    volumes:
      - ./templates:/app/templates
      - /mnt/telemetry/models/whisper/${WHISPER_MODEL_SIZE}:/root/.cache/huggingface/hub/models--distil-whisper--${WHISPER_MODEL_SIZE}
    environment:
      # Core service identity
      - PROJECT=${PROJECT}
      - SERVICE_NAME=${SERVICE_NAME}
      - SERVICE_VERSION=${SERVICE_VERSION}
      - HUB_PORT=${HUB_PORT}

      # Whisper + Brain (Legacy params kept for docker compat, though unused)
      - WHISPER_MODEL_SIZE=${WHISPER_MODEL_SIZE}
      - WHISPER_DEVICE=${WHISPER_DEVICE}
      - WHISPER_COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE}

      # Orion Bus
      - ORION_BUS_URL=${ORION_BUS_URL:-${ORION_BUS_URL}}
      - ORION_BUS_ENABLED=${ORION_BUS_ENABLED}

      # Channels
      - CHANNEL_VOICE_TRANSCRIPT=${CHANNEL_VOICE_TRANSCRIPT}
      - CHANNEL_VOICE_LLM=${CHANNEL_VOICE_LLM}
      - CHANNEL_VOICE_TTS=${CHANNEL_VOICE_TTS}

      - CHANNEL_COLLAPSE_INTAKE=${CHANNEL_COLLAPSE_INTAKE}
      - CHANNEL_COLLAPSE_TRIAGE=${CHANNEL_COLLAPSE_TRIAGE}

      - CHANNEL_CHAT_HISTORY_LOG=${CHANNEL_CHAT_HISTORY_LOG}

      # --- Cortex Gateway Integration (Dumb Hub) ---
      - CORTEX_GATEWAY_REQUEST_CHANNEL=${CORTEX_GATEWAY_REQUEST_CHANNEL}
      - CORTEX_GATEWAY_RESULT_PREFIX=${CORTEX_GATEWAY_RESULT_PREFIX}

      # --- TTS Integration ---
      - TTS_REQUEST_CHANNEL=${TTS_REQUEST_CHANNEL}
      - TTS_RESULT_PREFIX=${TTS_RESULT_PREFIX}

      # --- Runtimes ---
      - TIMEOUT_SEC=${TIMEOUT_SEC}
      - NVIDIA_VISIBLE_DEVICES=all

    runtime: nvidia

    # if running on athena:
    #networks:
    #  - ${NET}

networks:
  app-net:
    external: true
