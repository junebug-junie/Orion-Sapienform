# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ðŸ§  Orion LLM Gateway â€” bus-native model client
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SERVICE_NAME=llm-gateway
SERVICE_VERSION=0.1.0

# Bus (usually already defined at project root, but safe to override here)
ORION_BUS_ENABLED=true
ORION_BUS_URL=redis://100.92.216.81:6379/0

# LLM gateway bus channels
CHANNEL_LLM_INTAKE=orion-exec:request:LLMGatewayService
CHANNEL_LLM_REPLY_PREFIX=orion:llm:reply

# LLM defaults
ORION_DEFAULT_LLM_MODEL=llama3.1:8b-instruct-q8_0
ORION_LLM_DEFAULT_BACKEND=ollama

# Backend endpoints
# Today: Ollama running as `brain-llm` on app-net
ORION_LLM_OLLAMA_URL=http://100.121.214.30:11434

# Future: vLLM (OpenAI-style) on your V100 carrier board
# ORION_LLM_VLLM_URL=http://orion-vllm:8000

# Optional: call Brain's /chat (Spark, Ï†, collapse) as a backend
# ORION_LLM_BRAIN_URL=http://brain:8088

# Timeouts (seconds)
CONNECT_TIMEOUT_SEC=10
READ_TIMEOUT_SEC=60

# Service label in replies
ORION_LLM_SERVICE_NAME=LLMGatewayService
