# services/orion-vllm/.env

SERVICE_NAME=orion-vllm
SERVICE_VERSION=0.1.0

# HTTP bind
VLLM_HOST=0.0.0.0
VLLM_PORT=8000
VLLM_HOST_PORT=7000

# Which llm_profiles.yaml entry this node should run
#VLLM_PROFILE_NAME=llama3-7b-v100-1gpu
VLLM_PROFILE_NAME=mistral-7b-vllm

# Optional: override model_id from profile (usually leave blank)
VLLM_MODEL_ID=

# Runtime-only knobs
VLLM_GPU_MEMORY_FRACTION=0.9
VLLM_DOWNLOAD_DIR=/models
VLLM_ENFORCE_EAGER=false

# Path to profiles YAML *inside* the container
LLM_PROFILES_CONFIG_PATH=/app/config/llm_profiles.yaml
