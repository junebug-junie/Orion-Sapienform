# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ðŸ§  Orion Vector Host
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SERVICE_NAME=orion-vector-host
SERVICE_VERSION=0.1.0

ORION_BUS_ENABLED=true
ORION_BUS_ENFORCE_CATALOG=true
#ORION_BUS_URL=redis://orion-redis:6379/0

HEARTBEAT_INTERVAL_SEC=10

# Channels
VECTOR_HOST_CHAT_HISTORY_CHANNEL=orion:chat:history:log
VECTOR_HOST_CHAT_TURN_CHANNEL=orion:chat:history:turn
VECTOR_HOST_EMBEDDING_REQUEST_CHANNEL=orion:embedding:generate
VECTOR_HOST_EMBEDDING_RESULT_PREFIX=orion:embedding:result:
VECTOR_HOST_SEMANTIC_UPSERT_CHANNEL=orion:vector:semantic:upsert
VECTOR_HOST_SKIP_REJECTED=true
VECTOR_HOST_EMBED_DURABLE_ONLY=false

# Embedding config
VECTOR_HOST_EMBED_ROLES=["user","assistant"]
# This selects the provider vector-host uses to compute semantic embeddings for ALL texts
# (ollama/vllm/cola/llamacpp outputs). It does NOT mean only vLLM gets embedded.
VECTOR_HOST_EMBED_BACKEND=hf
VECTOR_HOST_EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
VECTOR_HOST_EMBEDDING_DEVICE=cpu
VECTOR_HOST_SEMANTIC_COLLECTION=orion_main_store
VECTOR_HOST_CHAT_MESSAGE_COLLECTION=orion_chat
VECTOR_HOST_CHAT_TURN_COLLECTION=orion_chat_turns

ORION_LLM_VLLM_URL=http://orion-vllm-host:8000
ORION_LLM_LLAMA_COLA_URL=http://orion-llama-cola-host:8004

VECTOR_HOST_EMBED_CONNECT_TIMEOUT_SEC=10
VECTOR_HOST_EMBED_READ_TIMEOUT_SEC=60

LOG_LEVEL=INFO
