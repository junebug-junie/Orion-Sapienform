# ───────────────────────────────────────────────────────────────
# Orion Hub — Voice + Web Gateway
# Pulls both root + local .env files
# ───────────────────────────────────────────────────────────────

x-env-context: &env
  env_file:
    - /mnt/services/Orion-Sapienform/.env      # global mesh env (PROJECT, NET, etc.)
    - .env                                    # local hub overrides

services:

  hub-app:
    <<: *env
    build:
      context: ../..
      dockerfile: services/orion-hub/Dockerfile
    container_name: ${PROJECT:-orion}-hub
    volumes:
      - ./templates:/app/templates
      # MODIFIED: Mount the pre-downloaded Whisper model into the default cache path
      - /mnt/telemetry/models/whisper/distil-medium.en:/root/.cache/huggingface/hub/models--distil-whisper--distil-medium.en
    ports:
      - "${HUB_PORT:-8080}:8080"
    depends_on:
      coqui-tts:
        condition: service_started
    restart: unless-stopped
    environment:
      # Whisper model settings - The app will find the model in the cache
      - WHISPER_MODEL_SIZE=${WHISPER_MODEL_SIZE}
      - WHISPER_DEVICE=${WHISPER_DEVICE}
      - WHISPER_COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE}

      # Cognitive backends
      - BRAIN_URL=${BRAIN_URL}
      - ORION_BUS_URL=${ORION_BUS_URL}
      - ORION_BUS_ENABLED=${ORION_BUS_ENABLED}

      # TTS service endpoint
      - TTS_URL=http://coqui-tts:5002/api/tts

      # Optional GPU access
      - NVIDIA_VISIBLE_DEVICES=all
    # REMOVED: The hardcoded DNS is no longer needed
    runtime: nvidia
    networks:
      - ${NET:-app-net}

  coqui-tts:
    <<: *env
    image: ghcr.io/coqui-ai/tts-cpu
    container_name: ${PROJECT:-orion}-hub-tts
    ports:
      - "5002:5002"
    volumes:
      - /mnt/telemetry/models/coqui/tts:/root/.local/share/tts
    environment:
      - TTS_MODEL_NAME=tts_models/en/ljspeech/vits
    restart: unless-stopped
    # This command ensures the container runs the web server process.
    entrypoint: tts-server
    networks:
      - ${NET:-app-net}

  caddy:
    <<: *env
    image: caddy:2-alpine
    container_name: ${PROJECT:-orion}-hub-caddy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./caddy/Caddyfile.${MODE:-dev}:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    depends_on:
      - hub-app
    networks:
      - ${NET:-app-net}

volumes:
  caddy_data:
  caddy_config:

networks:
  app-net:
    external: true
