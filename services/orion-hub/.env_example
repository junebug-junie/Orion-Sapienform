#--- Faster-Whisper Model Configuration ---
#Specifies the size of the Whisper model to use for transcription.
#Smaller models are faster but less accurate. Larger models are more accurate but require more VRAM.
#Recommended options: tiny.en, base.en, small.en, medium.en, distil-medium.en, large-v3
WHISPER_MODEL_SIZE=large-v3

#Specifies the device to run the Whisper model on.
#Use "cuda" for NVIDIA GPUs or "cpu" for the processor.
WHISPER_DEVICE=cuda

#Specifies the computation type for the model on the GPU.
#Using float16 is a good balance of speed and precision for most modern GPUs.
#Options: float16, int8_float16, int8\
WHISPER_COMPUTE_TYPE=float16

# mistral or mixtral
LLM_MODEL=mistral

# Brain service URL
BRAIN_URL=http://orion-brain:8088

# orion bus
ORION_BUS_ENABLED=true
ORION_BUS_URL=redis://orion-redis:6379/0
### Caddy 

# Mode switch (prod | dev)
MODE=dev
