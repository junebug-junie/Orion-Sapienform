services:
  llm-gateway:
    build:
      context: ../..
      dockerfile: services/orion-llm-gateway/Dockerfile
    container_name: ${PROJECT}-llm-gateway
    restart: unless-stopped
    networks:
      - app-net

    # Pull in the base .env (bus, model defaults, etc.)
    env_file:
      - .env

    environment:
      # Project-wide basics
      - PROJECT=${PROJECT}

      # Bus wiring (usually from root .env)
      - ORION_BUS_URL=${ORION_BUS_URL}
      - ORION_BUS_ENABLED=${ORION_BUS_ENABLED}

      # Identity
      - SERVICE_NAME=${SERVICE_NAME_LLM_GATEWAY:-llm-gateway}
      - SERVICE_VERSION=${SERVICE_VERSION_LLM_GATEWAY:-0.1.0}

      - HEARTBEAT_INTERVAL_SEC=${HEARTBEAT_INTERVAL_SEC}

      # LLM gateway channels
      - CHANNEL_LLM_INTAKE=${CHANNEL_LLM_INTAKE}
      - CHANNEL_LLM_REPLY_PREFIX=${CHANNEL_LLM_REPLY_PREFIX}
      - CHANNEL_INTROSPECT_CANDIDATE=orion:spark:introspect:candidate

      # Backend endpoints (Ollama is required today; others optional)
      - ORION_LLM_VLLM_URL=${ORION_LLM_VLLM_URL:-}
      - ORION_LLM_LLAMACPP_URL=${ORION_LLM_LLAMACPP_URL:-}

      # Timeouts
      - CONNECT_TIMEOUT_SEC=${CONNECT_TIMEOUT_SEC}
      - READ_TIMEOUT_SEC=${READ_TIMEOUT_SEC}

      # Label used in reply payloads
      - ORION_LLM_SERVICE_NAME=${ORION_LLM_SERVICE_NAME}

      # ─────────────────────────────────────
      # Profiles config (new)
      # ─────────────────────────────────────
      # Path inside the container
      - LLM_DEFAULT_PROFILE_NAME=${LLM_DEFAULT_PROFILE_NAME:-llama3-7b-v100-1gpu}
      - LLM_PROFILES_CONFIG_PATH=${LLM_PROFILES_CONFIG_PATH}
      - ORION_DEFAULT_LLM_MODEL=${ORION_DEFAULT_LLM_MODEL}
      - ORION_LLM_DEFAULT_BACKEND=${ORION_LLM_DEFAULT_BACKEND}

    ports:
      - "8210:8210"

    extra_hosts:
      - "host.docker.internal:host-gateway"

networks:
  app-net:
    external: true
