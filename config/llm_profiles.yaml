profiles:

  # ___ testing ___ #

  mistral-7b-vllm:
    display_name: "Mistral 7B – vLLM"
    task_type: chat
    backend: vllm
    model_id: "mistralai/Mistral-7B-Instruct-v0.3"
    supports_tools: false
    supports_embeddings: false
    supports_vision: false
    gpu:
      num_gpus: 2
      tensor_parallel_size: 2
      device_ids: [1, 3] # V100s on pcie
      max_model_len: 8192
      max_batch_tokens: 4096
      max_concurrent_requests: 4
      gpu_memory_fraction: 0.8
    notes: "Open, non-gated default for stack bring-up."

# ─────────────────────────────────────────────
# Dolphin 2.7 Mixtral 8x7B via llama.cpp
# ─────────────────────────────────────────────
dolphin-2.7-mixtral-8x7b-gguf:
  display_name: "Dolphin 2.7 – Mixtral 8x7B Q5_K_M (llama.cpp, Atlas)"
  task_type: chat
  backend: llamacpp

  # Logical model id (for our own labeling; llama.cpp mostly ignores it)
  model_id: "dolphin-2.7-mixtral-8x7b-q5_k_m"

  supports_tools: false
  supports_embeddings: false
  supports_vision: false

  gpu:
    # Intent: mid-to-heavy reasoning MoE model on Atlas via llama.cpp
    num_gpus: 4
    tensor_parallel_size: 4          # semantic only; llama.cpp handles GPU layers itself
    device_ids: [0, 1, 2, 3]               # adjust to whatever you map for orion-atlas-llamaccp

    # Orion’s conceptual view of this profile (not all directly enforced by llama.cpp)
    max_model_len: 8192              # match LLAMACPP_CTX_SIZE for this model
    max_batch_tokens: 512            # keep batches modest to avoid latency spikes
    max_concurrent_requests: 2       # MoE model; okay with a bit more concurrency than 70B
    gpu_memory_fraction: 0.9         # documentation hint only for llamacpp profiles

  notes: |
    Dolphin 2.7 Mixtral 8x7B (Q5_K_M GGUF) served by the orion-llamacpp service
    on Atlas, using llama.cpp server-cuda.

    Engine:
      - HTTP base URL: http://orion-atlas-llamaccp:8000
      - GGUF filename (example): /models/dolphin-2.7-mixtral-8x7b-Q5_K_M.gguf
      - Context: 8192 tokens (LLAMACPP_CTX_SIZE=8192)
      - N_GPU_LAYERS: tuned to fit Q5_K_M across 2× V100 16GB
        (e.g. N_GPU_LAYERS=80–120 depending on your tests)

    Orion usage:
      - General-purpose “deep-ish” chat / analysis model
      - Good candidate for council / reasoning verbs when 70B is overkill
      - Slightly higher concurrency than the DeepSeek 70B brain, but still
        not meant as the ultra-high-throughput casual chitchat model.


# ─────────────────────────────────────────────
# DeepSeek 70B via llama.cpp on Atlas
# ─────────────────────────────────────────────
deepseek-70b-gguf-atlas:
  display_name: "DeepSeek R1 70B – GGUF via llama.cpp (Atlas)"
  task_type: chat
  backend: llamacpp
  # Logical model id (for our own labeling; llama.cpp mostly ignores it)
  model_id: "deepseek-r1-70b-q4_k_m"

  supports_tools: false
  supports_embeddings: false     # you can flip to true if you use /v1/embeddings later
  supports_vision: false

  gpu:
    # Docs / intent: we’re dedicating 3 V100s on Atlas to this brain
    num_gpus: 3
    tensor_parallel_size: 1      # llama.cpp does its own thing; keep 1 for semantic clarity
    device_ids: [0, 1, 2]        # whichever GPUs you actually pin the container to

    # These shape how Orion *thinks* about using this profile
    # (llama.cpp has its own ctx/parallel flags, but this keeps everything aligned)
    max_model_len: 8192          # matches LLAMACPP_CTX_SIZE in .env
    max_batch_tokens: 512        # effectively “don’t slam this dragon”
    max_concurrent_requests: 1   # deep brain is low-concurrency, high-depth
    gpu_memory_fraction: 0.9     # conceptual; not directly used by llama.cpp, but good doc

  notes: |
    Deep reasoning / council brain for Orion on Atlas.
    Served by the orion-llamacpp service (llama.cpp server-cuda) on app-net.

    Engine:
      - HTTP base URL: http://orion-llamacpp:8000
      - GGUF: /models/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf
      - Context: 8192 tokens (LLAMACPP_CTX_SIZE)
      - N_GPU_LAYERS: 80 (tuned for 3× V100 16GB)

    Orion usage:
      - Single active request at a time (max_concurrent_requests=1)
      - Small-ish batches (max_batch_tokens=512) to avoid latency spikes
      - Intended for deep/council/spark verbs, not casual chitchat.

  # ─────────────────────────────────────────────
  # 1) Llama3 7B on 1× V100 16GB
  #    - Stable default; low complexity
  # ─────────────────────────────────────────────
  llama3-7b-v100-1gpu:
    display_name: "Llama 3 7B – 1× V100"
    task_type: chat
    backend: vllm
    model_id: "meta-llama/Meta-Llama-3-8B-Instruct"
    supports_tools: false
    supports_embeddings: false
    supports_vision: false
    gpu:
      num_gpus: 1
      tensor_parallel_size: 1
      device_ids: [3]
      max_model_len: 8192
      max_batch_tokens: 4096
      max_concurrent_requests: 4
      gpu_memory_fraction: 0.9
    preferred_verbs:
      - generic_chat
      - status_report
      - short_explain
    notes: "Default vLLM profile: 1× V100, safe context/throughput."

  # ─────────────────────────────────────────────
  # 2) Llama3 7B on 2× V100 16GB
  #    - More throughput; same model
  # ─────────────────────────────────────────────
  llama3-7b-v100-2gpu:
    display_name: "Llama 3 7B – 2× V100"
    task_type: chat
    backend: vllm
    model_id: "meta-llama/Meta-Llama-3-8B-Instruct"
    supports_tools: false
    supports_embeddings: false
    supports_vision: false
    gpu:
      num_gpus: 2
      tensor_parallel_size: 2
      device_ids: [0, 2]
      max_model_len: 8192     # bump later if stable
      max_batch_tokens: 8192
      max_concurrent_requests: 8
      gpu_memory_fraction: 0.
    notes: "Heavier analysis / narrative workloads; more parallelism."

  # ─────────────────────────────────────────────
  # 3) Llama3 7B on 4× V100 16GB
  #    - High concurrency / multi-session
  # ─────────────────────────────────────────────
  llama3-7b-v100-4gpu:
    display_name: "Llama 3 7B – 4× V100"
    task_type: chat
    backend: vllm
    model_id: "meta-llama/Llama-3.1-8B-Instruct"
    supports_tools: false
    supports_embeddings: false
    supports_vision: false
    gpu:
      num_gpus: 4
      tensor_parallel_size: 4
      device_ids: [0, 1, 2, 3]
      max_model_len: 16000
      max_batch_tokens: 16384
      max_concurrent_requests: 16
      gpu_memory_fraction: 0.9
    notes: "High throughput profile when node is dedicated to Orion."

  # ─────────────────────────────────────────────
  # 4) Llama3 70B on 4× V100 16GB
  #    - FUTURE NON Volta Arch: Big-brain mode for specific verbs
  # ─────────────────────────────────────────────
  llama3-70b-v100-4gpu:
    display_name: "Llama 3 70B – 4× V100"
    task_type: chat
    backend: vllm
    model_id: "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4"
    supports_tools: false
    supports_embeddings: false
    supports_vision: false
    gpu:
      num_gpus: 4
      tensor_parallel_size: 4
      device_ids: [0, 1, 2, 3]
      max_model_len: 4096       # be conservative for 70B
      max_batch_tokens: 4096
      max_concurrent_requests: 2
      gpu_memory_fraction: 0.8
    notes: "High-cost, high-capacity profile reserved for select verbs."

  # ─────────────────────────────────────────────
  # DEEPER Brain options
  #  
  # ─────────────────────────────────────────────

  mixtral-8x7b-deep:
    display_name: "Mixtral 8x7B Instruct (Atlas Deep)"
    task_type: chat
    backend: vllm
    model_id: mistralai/Mixtral-8x7B-Instruct-v0.1

    gpu:
      num_gpus: 4
      tensor_parallel_size: 4
      device_ids: [0, 1, 2, 3]
      max_model_len: 4500 #8192
      max_batch_tokens: 2000 #null
      max_concurrent_requests: 2
      gpu_memory_fraction: 0.7

    supports_tools: true
    supports_embeddings: false
    supports_vision: false

    notes: "High-quality MoE model; roughly Llama2-70B-level capability. Uses all 4× V100 on Atlas via tensor parallelism."

  qwen25-32b-deep:
    display_name: "Qwen2.5 32B Instruct (Atlas Deep)"
    task_type: chat
    backend: vllm
    model_id: Qwen/Qwen2.5-32B-Instruct

    gpu:
      num_gpus: 4
      tensor_parallel_size: 4
      device_ids: [0, 1, 2, 3]
      max_model_len: 4096        # keep this sane for V100 VRAM
      max_batch_tokens: null
      max_concurrent_requests: 2 # keep small – this is your “solo guru” brain
      gpu_memory_fraction: 0.78

    supports_tools: true
    supports_embeddings: false
    supports_vision: false

    notes: |
      Dense 32B model with strong reasoning & JSON skills.
      Tight but workable on 4× 16GB V100; treat it as a low-concurrency deep brain.


  qwen25-14b-main:
    display_name: "Qwen2.5 14B Instruct (Main)"
    task_type: chat
    backend: vllm
    model_id: Qwen/Qwen2.5-14B-Instruct

    gpu:
      num_gpus: 2
      tensor_parallel_size: 2
      max_model_len: 8192
      device_ids: [0, 1, 2, 3]
      max_batch_tokens: null
      max_concurrent_requests: 8
      gpu_memory_fraction: 0.8

    supports_tools: true
    supports_embeddings: false
    supports_vision: false

    notes: |
      Strong generalist. Feels more like a grown-up than 8B models.
      Leave Mixtral or Qwen2.5-32B for the heavy council / deep modes.


  # ─────────────────────────────────────────────
  # 5) Embeddings profile (example)
  #    - For text embeddings via vLLM (if you enable them)
  # ─────────────────────────────────────────────
  llama3-embed-v100-1gpu:
    display_name: "Llama Embeddings – 1× V100"
    task_type: embeddings
    backend: vllm
    model_id: "text-embedding-model-id"  # <-- actual embedding model
    supports_tools: false
    supports_embeddings: true
    supports_vision: false
    gpu:
      num_gpus: 1
      tensor_parallel_size: 1
      devices_ids: [4]
      max_model_len: 2048
      max_batch_tokens: 8192
      max_concurrent_requests: 8
      gpu_memory_fraction: 0.9
    notes: "Embeddings-only profile for vector stores."
