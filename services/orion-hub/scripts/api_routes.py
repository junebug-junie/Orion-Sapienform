# services/orion-hub/scripts/api_routes.py
from __future__ import annotations

import logging
from typing import Optional, Any, List, Dict, Tuple

from fastapi import APIRouter, Header
from fastapi.responses import HTMLResponse, JSONResponse

from .settings import settings
from .warm_start import mini_personality_summary
from .llm_rpc import BrainRPC, CouncilRPC
from .recall_rpc import RecallRPC
from .session import ensure_session
from orion.schemas.collapse_mirror import CollapseMirrorEntry

logger = logging.getLogger("orion-hub.api")

router = APIRouter()


# ======================================================================
# ğŸ  ROOT + STATIC HTML
# ======================================================================
@router.get("/")
async def root():
    """Serves the main Hub UI (index.html)."""
    from .main import html_content
    return HTMLResponse(content=html_content, status_code=200)


@router.get("/health")
def health():
    """Simple health check endpoint."""
    return {"status": "ok", "service": settings.SERVICE_NAME}


# ======================================================================
# ğŸ§  SESSION MANAGEMENT
# ======================================================================
@router.get("/api/session")
async def api_session(x_orion_session_id: Optional[str] = Header(None)):
    """
    Called by Hub UI on load.
    Always returns a warm-started session_id.
    """
    from .main import bus
    if not bus:
        raise RuntimeError("OrionBus not initialized.")

    session_id = await ensure_session(x_orion_session_id, bus)
    return {"session_id": session_id}


# ======================================================================
# ğŸ” RECALL â†’ MEMORY DIGEST HELPERS
# ======================================================================

def _format_fragments_for_digest(
    fragments: List[Dict[str, Any]],
    limit: int = 8,
) -> str:
    """
    Turn raw recall fragments into a compact bullet list for an internal
    'memory digest' LLM call.
    """
    lines: List[str] = []
    for f in fragments[:limit]:
        kind = f.get("kind", "unknown")
        source = f.get("source", "unknown")
        text = (f.get("text") or "").replace("\n", " ").strip()

        meta = f.get("meta") or {}
        observer = meta.get("observer")
        field_resonance = meta.get("field_resonance")

        extras: List[str] = []
        if observer:
            extras.append(f"observer={observer}")
        if field_resonance:
            extras.append(f"field_resonance={field_resonance}")

        suffix = f" [{' | '.join(extras)}]" if extras else ""
        lines.append(f"- [{kind}/{source}] {text}{suffix}")

    return "\n".join(lines)


async def build_memory_digest(
    bus,
    session_id: str,
    user_prompt: str,
    chat_mode: str = "brain",
    max_items: int = 12,
) -> Tuple[str, Dict[str, Any]]:
    """
    1) Call Recall over the Orion bus.
    2) Ask Brain (via BrainRPC) to condense fragments into 3â€“5 bullets.
    3) Return (digest_text, recall_debug) for use as internal context.

    This leans on the Recall service's semantic+salience+recency scoring;
    hub stays intentionally dumb and only does LLM summarization.
    """
    # Choose recall mode/window based on chat mode
    if chat_mode == "council":
        recall_mode = "deep"
        time_window_days = 90
    else:
        recall_mode = "hybrid"
        time_window_days = 30

    recall_client = RecallRPC(bus)
    recall_result = await recall_client.call_recall(
        query=user_prompt,
        session_id=session_id,
        mode=recall_mode,
        time_window_days=time_window_days,
        max_items=max_items,
        extras=None,
    )

    fragments = recall_result.get("fragments") or []
    debug = recall_result.get("debug") or {}

    if not fragments:
        logger.info("build_memory_digest: no fragments returned from recall.")
        return "", {
            "total_fragments": 0,
            "mode": recall_mode,
            "time_window_days": time_window_days,
        }

    fragments_block = _format_fragments_for_digest(fragments)
    if not fragments_block:
        return "", {
            "total_fragments": len(fragments),
            "mode": recall_mode,
            "time_window_days": time_window_days,
        }

    # 2) Memory digest via BrainRPC (still on the bus)
    system = (
        "You are OrÃ­on, Juniper's collaborative AI co-journeyer.\n"
        "You will receive:\n"
        "1) The user's current message.\n"
        "2) A small list of past events and dialogues ('fragments').\n\n"
        "Your job:\n"
        "- Identify ONLY the 3â€“5 most relevant threads for understanding and responding to the current message.\n"
        "- Return them as short bullet points.\n"
        "- Each bullet should be one sentence.\n"
        "- Do not include anything unrelated.\n"
        "- This is internal memory context; the user will not see this directly.\n"
    )

    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": f"Current message: {user_prompt}"},
        {
            "role": "user",
            "content": "Relevant memory fragments:\n" + fragments_block,
        },
    ]

    rpc = BrainRPC(bus, kind="memory_digest")
    reply = await rpc.call_llm(
        prompt=user_prompt,
        history=messages,
        temperature=0.0,
    )
    text = (reply.get("text") or reply.get("response") or "").strip()
    logger.info("build_memory_digest: got digest length=%d", len(text))

    # merge recall debug with basic info
    debug_out = {
        "total_fragments": len(fragments),
        "mode": debug.get("mode", recall_mode),
        "time_window_days": time_window_days,
        "max_items": max_items,
        "note": debug.get("note", "semantic+salience+recency scoring"),
    }

    return text, debug_out


# ======================================================================
# ğŸ’¬ SHARED CHAT CORE (HTTP + WS)
# ======================================================================

async def handle_chat_request(
    bus,
    payload: dict,
    session_id: str,
) -> Dict[str, Any]:
    """
    Core chat handler used by both HTTP /api/chat and (optionally) WebSocket.

    - Handles Brain vs Council routing.
    - Optionally pulls a Recall â†’ Brain memory digest when payload.use_recall is true.
    - Injects mini_personality_summary + digest as a single system stub.
    """
    user_messages = payload.get("messages", [])
    temperature = payload.get("temperature", 0.7)
    mode = payload.get("mode", "brain")  # "brain" | "council"
    use_recall = bool(payload.get("use_recall", False))

    if not isinstance(user_messages, list) or len(user_messages) == 0:
        return {"error": "Invalid payload: missing messages[]"}

    user_prompt = user_messages[-1].get("content", "") or ""

    # Optional: Recall â†’ Digest
    memory_digest = ""
    recall_debug: Dict[str, Any] = {}
    if use_recall:
        try:
            memory_digest, recall_debug = await build_memory_digest(
                bus=bus,
                session_id=session_id,
                user_prompt=user_prompt,
                chat_mode=mode,
                max_items=12,
            )
        except Exception as e:
            logger.warning("Memory digest failed: %s", e, exc_info=True)
            memory_digest = ""
            recall_debug = {"error": str(e)}

    # Build system stub = personality + optional memory digest
    system_content = mini_personality_summary()
    if memory_digest:
        system_content += "\n\nInternal memory context:\n" + memory_digest

    system_stub = {"role": "system", "content": system_content}
    full_history = [system_stub] + user_messages

    # Choose backend
    if mode == "council":
        rpc = CouncilRPC(bus)
    else:
        rpc = BrainRPC(bus)

    reply = await rpc.call_llm(
        prompt=user_prompt,
        history=full_history,
        temperature=temperature,
    )

    text = reply.get("text") or reply.get("response") or ""
    tokens = len(text.split()) if text else 0

    return {
        "session_id": session_id,
        "mode": mode,
        "use_recall": use_recall,
        "text": text,
        "tokens": tokens,
        "raw": reply,
        "recall_debug": recall_debug,
    }


# ======================================================================
# ğŸ’¬ CHAT ENDPOINT (HTTP wrapper around core)
# ======================================================================
@router.post("/api/chat")
async def api_chat(
    payload: dict,
    x_orion_session_id: Optional[str] = Header(None),
):
    """
    Main LLM chat endpoint.

    - Preserves warm-started sessions + personality stubs.
    - Uses BrainRPC by default.
    - If payload.mode == "council", routes through Agent Council instead.
    - If payload.use_recall == true, pulls a semantic/salience/recency-weighted
      memory digest from Recall â†’ Brain and injects it into the system stub.
    """
    from .main import bus
    if not bus:
        raise RuntimeError("OrionBus not initialized.")

    # Ensure warm-started session
    session_id = await ensure_session(x_orion_session_id, bus)

    result = await handle_chat_request(bus, payload, session_id)

    # Handle simple validation errors
    if "error" in result:
        return JSONResponse(status_code=400, content=result)

    text = result.get("text") or ""
    user_messages = payload.get("messages") or []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ§¾ Store chat tail in Redis (last 20 entries)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    client = getattr(bus, "client", None)
    if client is not None and user_messages:
        try:
            history_key = f"orion:hub:session:{session_id}:history"
            client.lpush(history_key, str(user_messages[-1])[:4000])
            client.ltrim(history_key, 0, 19)
        except Exception as e:
            logger.warning(
                "Failed to store chat tail in Redis for %s: %s",
                session_id,
                e,
            )

    return result


# ======================================================================
# ğŸ” RECALL / RAG ENDPOINT (bus faÃ§ade)
# ======================================================================
@router.post("/api/recall")
async def api_recall(
    payload: dict,
    x_orion_session_id: Optional[str] = Header(None),
):
    """
    Thin HTTP faÃ§ade over the Recall service (via Orion Bus).

    Expects payload like:
        {
          "query": "string (optional)",
          "mode": "hybrid|short_term|deep (optional)",
          "time_window_days": 30,
          "max_items": 16,
          "extras": {...}   # optional hints/filters
        }

    This does NOT call Recall over HTTP â€“ it uses RecallRPC on the bus,
    same as /api/chat, so all intra-Orion cognition stays on the spine.

    Scoring is handled inside the Recall service (semantic + salience + recency).
    """
    from .main import bus
    if not bus:
        raise RuntimeError("OrionBus not initialized.")

    session_id = await ensure_session(x_orion_session_id, bus)

    query = payload.get("query") or ""
    mode = payload.get("mode") or "hybrid"
    time_window_days = payload.get("time_window_days") or 30
    max_items = payload.get("max_items") or 16
    extras = payload.get("extras") or None

    client = RecallRPC(bus)

    result = await client.call_recall(
        query=query,
        session_id=session_id,
        mode=mode,
        time_window_days=time_window_days,
        max_items=max_items,
        extras=extras,
    )

    return {
        "session_id": session_id,
        "query": query,
        "mode": mode,
        "time_window_days": time_window_days,
        "max_items": max_items,
        "result": result,
    }


# ======================================================================
# ğŸ“¿ COLLAPSE MIRROR ENDPOINTS
# ======================================================================
@router.get("/schema/collapse")
def get_collapse_schema():
    """Exposes the CollapseMirrorEntry schema for UI templating."""
    logger.info("Fetching CollapseMirrorEntry schema")
    return JSONResponse(CollapseMirrorEntry.schema())


@router.post("/submit-collapse")
async def submit_collapse(data: dict):
    """
    Receives Collapse Mirror data and publishes it to the bus.
    """
    from .main import bus
    logger.info(f"ğŸ”¥ /submit-collapse called with: {data}")

    if not bus or not bus.enabled:
        logger.error("Submission failed: OrionBus is disabled or not connected.")
        return JSONResponse(
            status_code=503,
            content={"success": False, "error": "OrionBus disabled or unavailable"},
        )

    try:
        entry = CollapseMirrorEntry(**data).with_defaults()
        bus.publish(settings.CHANNEL_COLLAPSE_INTAKE, entry.model_dump(mode="json"))
        logger.info(
            "ğŸ“¡ Published Collapse Mirror â†’ %s",
            settings.CHANNEL_COLLAPSE_INTAKE,
        )

        return {"success": True}

    except Exception as e:
        logger.error(f"âŒ Hub publish error: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": str(e)},
        )
