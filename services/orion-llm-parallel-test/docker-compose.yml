services:
  # 1. Model Loader (Python API Version - Robust)
  llm-gpu-sync-test-model-loader:
    image: python:3.11-slim
    container_name: llm-gpu-sync-test-model-loader
    volumes:
      - "${LLM_CACHE_DIR}:/models"
    command: >
      bash -c "pip install --no-cache-dir huggingface_hub &&
      python -c \"import os;
      from huggingface_hub import hf_hub_download;
      file_path = '/models/gguf/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf';

      if os.path.exists(file_path):
          print('--- Model found. Skipping download. ---');
      else:
          print('--- Model not found. Downloading DeepSeek 70B... ---');
          hf_hub_download(
              repo_id='unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF',
              filename='DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf',
              local_dir='/models/gguf',
              local_dir_use_symlinks=False
          );
          print('--- Download Complete. ---');\""
  # 2. The API Router
  llm-gpu-sync-test-router:
    container_name: llm-gpu-sync-test-router
    build: .
    ports:
      - "${APP_PORT}:9000"
    env_file: .env
    depends_on:
      - llm-gpu-sync-test-llama-cpp
    restart: always

  # 3. llama.cpp Engine
  llm-gpu-sync-test-llama-cpp:
    container_name: llm-gpu-sync-test-llama-cpp
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    runtime: nvidia
    restart: unless-stopped
    environment:
      - CUDA_VISIBLE_DEVICES=0,1,2
    volumes:
      - "${LLM_CACHE_DIR}:/models"
    ports:
      # Maps your .env port (8082) to container internal 8080
      - "${LLAMA_CPP_PORT}:8080"
    depends_on:
      llm-gpu-sync-test-model-loader:
        condition: service_completed_successfully
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 3
              capabilities: [gpu]
    command: >
      -m /models/${LLAMA_CPP_MODEL_FILE}
      --host 0.0.0.0
      --port 8080
      -ngl 99
      -c ${CTX_SIZE}
      -fa
      --split-mode layer
      --main-gpu 0
