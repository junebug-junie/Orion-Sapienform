SERVICE_NAME=orion-llamacpp-neural-host
SERVICE_VERSION=0.1.0

# Which profile this service should boot
LLM_PROFILES_CONFIG_PATH=/app/config/llm_profiles.yaml
LLM_PROFILE_NAME=llama3-8b-instruct-q4km-athena-p4
#qwen2.5-32b-instruct-abliterated-gguf
#deepseek-70b-gguf-atlas

# Container mount root for models volume (compose mounts host cache -> /models)
MODELS_MOUNT_ROOT=/models
LLM_CACHE_DIR=/mnt/telemetry/llm-cache

# Optional: HuggingFace token (only needed for gated/private)
HF_TOKEN=
hf_token=

# Optional: runtime overrides (ONLY if you want to override the profile YAML)
# (These are your “safe boot” levers if the model crashes at 8192/parallel/etc.)
LLAMACPP_HOST_OVERRIDE=
LLAMACPP_PORT_OVERRIDE=

LLAMACPP_CTX_SIZE_OVERRIDE=
LLAMACPP_N_GPU_LAYERS_OVERRIDE=
LLAMACPP_THREADS_OVERRIDE=
LLAMACPP_N_PARALLEL_OVERRIDE=
LLAMACPP_BATCH_SIZE_OVERRIDE=

# Optional: override GPU pinning (otherwise uses profile.gpu.device_ids)
CUDA_VISIBLE_DEVICES_OVERRIDE=

# Behavior
ENSURE_MODEL_DOWNLOAD=true
WAIT_FOR_MODEL_SECONDS=0

# Host port mapping (used by compose for published port)
LLAMACPP_HOST_PORT=8005
ORION_BUS_ENFORCE_CATALOG=true
