profiles:

  # ___ testing ___ #

  mistral-7b-vllm:
    display_name: "Mistral 7B – vLLM"
    task_type: chat
    backend: vllm
    model_id: "mistralai/Mistral-7B-Instruct-v0.3"
    supports_tools: false
    supports_embeddings: false
    supports_vision: false
    gpu:
      num_gpus: 2
      tensor_parallel_size: 2
      device_ids: [1, 3] # V100s on pcie
      max_model_len: 8192
      max_batch_tokens: 4096
      max_concurrent_requests: 4
      gpu_memory_fraction: 0.8
    notes: "Open, non-gated default for stack bring-up."


  llama3-8b-instruct-q4km-athena-p4:
    display_name: "Llama 3 8B Instruct – Q4_K_M (llama.cpp/llama-cpp-python, Athena P4)"
    task_type: chat
    backend: llamacpp

    # Logical label used by Orion; your llamacpp wrapper can resolve this however you prefer
    model_id: "llama-3-8b-instruct-q4_k_m"

    supports_tools: false
    supports_embeddings: false
    supports_vision: false

    gpu:
      num_gpus: 1
      tensor_parallel_size: 1
      device_ids: [0]

      # P4-safe defaults
      max_model_len: 4096
      max_batch_tokens: 256
      max_concurrent_requests: 1
      gpu_memory_fraction: 0.85

    llamacpp:
      model_root: "/models/gguf"

      hf_repo_id: "PawanKrd/Meta-Llama-3-8B-Instruct-GGUF"
      hf_filename: "llama-3-8b-instruct.Q4_K_M.gguf"  # exists in repo :contentReference[oaicite:1]{index=1}

      host: "0.0.0.0"
      port: 8080

      # P4-safe runtime knobs
      ctx_size: 4096
      n_gpu_layers: 35     # 8B has 32 blocks; 35 usually means "everything it can"
      threads: 8
      n_parallel: 1
      batch_size: 256

    notes: |
      P4-friendly Llama 3 8B Instruct via GGUF Q4_K_M.
      If you OOM:
        - drop ctx_size to 2048
        - or lower n_gpu_layers
        - or use a smaller quant from the same repo.

  llama3-1-cola:
    display_name: "Llama 3.1 CoLA – Intention Host"
    task_type: chat
    backend: llama-cola
    model_id: "LAMDA-RL/Llama-3.1-CoLA-10B"
    supports_tools: false
    supports_embeddings: true
    supports_vision: false
    gpu:
      num_gpus: 1
      tensor_parallel_size: 1
      device_ids: [0]
      max_model_len: 4096
      max_batch_tokens: 256
      max_concurrent_requests: 1
      gpu_memory_fraction: 0.85
    llama_cola:
      model_root: "/models/cola"
      repo_id: "LAMDA-RL/Llama-3.1-CoLA-10B"
      host: "0.0.0.0"
      port: 8005
      max_new_tokens: 256
    notes: |
      CoLA inference host profile that returns both text and latent action paths.


  qwen2.5-32b-instruct-abliterated-gguf:
    display_name: "Qwen 2.5 32B Instruct – abliterated Q6_K (llama.cpp, Atlas)"
    task_type: chat
    backend: llamacpp

    # Logical label used by Orion; the llamacpp wrapper will resolve to a GGUF path
    model_id: "qwen2.5-32b-instruct-abliterated-q6_k"

    supports_tools: false
    supports_embeddings: false
    supports_vision: false

    gpu:
      num_gpus: 4
      tensor_parallel_size: 4      # semantic only for llamacpp
      device_ids: [0, 1, 2, 3]

      max_model_len: 8192
      max_batch_tokens: 512
      max_concurrent_requests: 2
      gpu_memory_fraction: 0.9

    llamacpp:
      model_root: "/models/gguf"

      hf_repo_id: "mradermacher/Qwen2.5-32B-Instruct-abliterated-GGUF"
      hf_filename: "Qwen2.5-32B-Instruct-abliterated.Q6_K.gguf"

      host: "0.0.0.0"
      port: 8080
      ctx_size: 8192
      n_gpu_layers: 80
      threads: 16
      n_parallel: 2
      batch_size: 512

    notes: |
      Non-coder Qwen2.5 32B Instruct (abliterated) GGUF via llama.cpp on Atlas.
      Source:
        - repo: mradermacher/Qwen2.5-32B-Instruct-abliterated-GGUF
        - file: Qwen2.5-32B-Instruct-abliterated.Q6_K.gguf


  # ─────────────────────────────────────────────
  # DeepSeek 70B via llama.cpp on Atlas
  # ─────────────────────────────────────────────
  deepseek-70b-gguf-atlas:
    display_name: "DeepSeek R1 70B – GGUF via llama.cpp (Atlas)"
    task_type: chat
    backend: llamacpp

    # Make model_id a concrete path the wrapper can load immediately
    model_id: "/models/gguf/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf"

    supports_tools: false
    supports_embeddings: false
    supports_vision: false

    gpu:
      num_gpus: 4
      tensor_parallel_size: 4
      device_ids: [0, 1, 2, 3]
      max_model_len: 8192
      max_batch_tokens: 512
      max_concurrent_requests: 1
      gpu_memory_fraction: 0.9

    llamacpp:
      model_root: "/models/gguf"

      # Leave these absent or null; model_id path is authoritative
      # hf_repo_id: ""
      # hf_filename: ""

      host: "0.0.0.0"
      port: 8080
      ctx_size: 8192
      n_gpu_layers: 80
      threads: 16
      n_parallel: 2
      batch_size: 512

  # ─────────────────────────────────────────────
  # 2) Llama3 7B on 2× V100 16GB
  #    - More throughput; same model
  # ─────────────────────────────────────────────
  llama3-7b-v100-2gpu:
    display_name: "Llama 3 7B – 2× V100"
    task_type: chat
    backend: vllm
    model_id: "meta-llama/Meta-Llama-3-8B-Instruct"
    supports_tools: false
    supports_embeddings: false
    supports_vision: false
    gpu:
      num_gpus: 2
      tensor_parallel_size: 2
      device_ids: [0, 2]
      max_model_len: 8192     # bump later if stable
      max_batch_tokens: 8192
      max_concurrent_requests: 8
      gpu_memory_fraction: 0.
    notes: "Heavier analysis / narrative workloads; more parallelism."

  # ─────────────────────────────────────────────
  # 3) Llama3 7B on 4× V100 16GB
  #    - High concurrency / multi-session
  # ─────────────────────────────────────────────
  llama3-7b-v100-4gpu:
    display_name: "Llama 3 7B – 4× V100"
    task_type: chat
    backend: vllm
    model_id: "meta-llama/Llama-3.1-8B-Instruct"
    supports_tools: false
    supports_embeddings: false
    supports_vision: false
    gpu:
      num_gpus: 4
      tensor_parallel_size: 4
      device_ids: [0, 1, 2, 3]
      max_model_len: 16000
      max_batch_tokens: 16384
      max_concurrent_requests: 16
      gpu_memory_fraction: 0.9
    notes: "High throughput profile when node is dedicated to Orion."

  # ─────────────────────────────────────────────
  # 4) Llama3 70B on 4× V100 16GB
  #    - FUTURE NON Volta Arch: Big-brain mode for specific verbs
  # ─────────────────────────────────────────────
  llama3-70b-v100-4gpu:
    display_name: "Llama 3 70B – 4× V100"
    task_type: chat
    backend: vllm
    model_id: "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4"
    supports_tools: false
    supports_embeddings: false
    supports_vision: false
    gpu:
      num_gpus: 4
      tensor_parallel_size: 4
      device_ids: [0, 1, 2, 3]
      max_model_len: 4096       # be conservative for 70B
      max_batch_tokens: 4096
      max_concurrent_requests: 2
      gpu_memory_fraction: 0.8
    notes: "High-cost, high-capacity profile reserved for select verbs."

  # ─────────────────────────────────────────────
  # DEEPER Brain options
  #  
  # ─────────────────────────────────────────────

  mixtral-8x7b-deep:
    display_name: "Mixtral 8x7B Instruct (Atlas Deep)"
    task_type: chat
    backend: vllm
    model_id: mistralai/Mixtral-8x7B-Instruct-v0.1

    gpu:
      num_gpus: 4
      tensor_parallel_size: 4
      device_ids: [0, 1, 2, 3]
      max_model_len: 4500 #8192
      max_batch_tokens: 2000 #null
      max_concurrent_requests: 2
      gpu_memory_fraction: 0.7

    supports_tools: true
    supports_embeddings: false
    supports_vision: false

    notes: "High-quality MoE model; roughly Llama2-70B-level capability. Uses all 4× V100 on Atlas via tensor parallelism."

  qwen25-32b-deep:
    display_name: "Qwen2.5 32B Instruct (Atlas Deep)"
    task_type: chat
    backend: vllm
    model_id: Qwen/Qwen2.5-32B-Instruct

    gpu:
      num_gpus: 4
      tensor_parallel_size: 4
      device_ids: [0, 1, 2, 3]
      max_model_len: 4096        # keep this sane for V100 VRAM
      max_batch_tokens: null
      max_concurrent_requests: 2 # keep small – this is your “solo guru” brain
      gpu_memory_fraction: 0.78

    supports_tools: true
    supports_embeddings: false
    supports_vision: false

    notes: |
      Dense 32B model with strong reasoning & JSON skills.
      Tight but workable on 4× 16GB V100; treat it as a low-concurrency deep brain.


  qwen25-14b-main:
    display_name: "Qwen2.5 14B Instruct (Main)"
    task_type: chat
    backend: vllm
    model_id: Qwen/Qwen2.5-14B-Instruct

    gpu:
      num_gpus: 2
      tensor_parallel_size: 2
      max_model_len: 8192
      device_ids: [0, 1, 2, 3]
      max_batch_tokens: null
      max_concurrent_requests: 8
      gpu_memory_fraction: 0.8

    supports_tools: true
    supports_embeddings: false
    supports_vision: false

    notes: |
      Strong generalist. Feels more like a grown-up than 8B models.
      Leave Mixtral or Qwen2.5-32B for the heavy council / deep modes.


  # ─────────────────────────────────────────────
  # 5) Embeddings profile (example)
  #    - For text embeddings via vLLM (if you enable them)
  # ─────────────────────────────────────────────
  llama3-embed-v100-1gpu:
    display_name: "Llama Embeddings – 1× V100"
    task_type: embeddings
    backend: vllm
    model_id: "text-embedding-model-id"  # <-- actual embedding model
    supports_tools: false
    supports_embeddings: true
    supports_vision: false
    gpu:
      num_gpus: 1
      tensor_parallel_size: 1
      devices_ids: [4]
      max_model_len: 2048
      max_batch_tokens: 8192
      max_concurrent_requests: 8
      gpu_memory_fraction: 0.9
    notes: "Embeddings-only profile for vector stores."
