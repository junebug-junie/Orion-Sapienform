# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ðŸ§  Orion Vector Host
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SERVICE_NAME=orion-vector-host
SERVICE_VERSION=0.1.0

ORION_BUS_ENABLED=true
ORION_BUS_ENFORCE_CATALOG=true
ORION_BUS_URL=redis://orion-redis:6379/0

HEARTBEAT_INTERVAL_SEC=10

# Channels
VECTOR_HOST_CHAT_HISTORY_CHANNEL=orion:chat:history:log
VECTOR_HOST_EMBEDDING_REQUEST_CHANNEL=orion:embedding:generate
VECTOR_HOST_EMBEDDING_RESULT_PREFIX=orion:embedding:result:
VECTOR_HOST_SEMANTIC_UPSERT_CHANNEL=orion:vector:semantic:upsert

# Embedding config
VECTOR_HOST_EMBED_ROLES=["user","assistant"]
# This selects the provider vector-host uses to compute semantic embeddings for ALL texts
# (ollama/vllm/cola/llamacpp outputs). It does NOT mean only vLLM gets embedded.
VECTOR_HOST_EMBED_BACKEND=vllm
VECTOR_HOST_EMBEDDING_MODEL=your-embedding-model
VECTOR_HOST_SEMANTIC_COLLECTION=orion_main_store

ORION_LLM_VLLM_URL=http://orion-vllm-host:8000
ORION_LLM_LLAMA_COLA_URL=http://orion-llama-cola-host:8004

VECTOR_HOST_EMBED_CONNECT_TIMEOUT_SEC=10
VECTOR_HOST_EMBED_READ_TIMEOUT_SEC=60

LOG_LEVEL=INFO
